{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_warn_always(False)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of datasets, we are combining Stanford SQuAD, Wiki QA and Trivia QA datasets.\n",
    "\n",
    "Common features have to be merged in order to ensure data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ad9c3ac8944f63b4ba3899d2d49614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57511a978694407ac1ba43502cb3618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d068c6a52ac740fc9f4a15e836559317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
      "{'question': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'answers': {'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}\n",
      "{'question': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'answers': {'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets, Features, Value, Sequence\n",
    "\n",
    "# Preprocess SQuAD\n",
    "def preprocess_squad(examples):\n",
    "    return {\n",
    "        \"id\": examples[\"id\"],\n",
    "        \"question\": examples[\"question\"],\n",
    "        \"context\": examples[\"context\"],\n",
    "        \"answers\": \n",
    "            {\"text\": examples[\"answers\"][\"text\"], \"answer_start\": [int(start) for start in examples[\"answers\"][\"answer_start\"]]}\n",
    "    }\n",
    "\n",
    "# Preprocess WikiQA\n",
    "def preprocess_wikiqa(examples):\n",
    "    # answers = {\"text\": examples[\"answer\"], \"answer_start\": [0] * len(examples[\"answer\"])}\n",
    "    answers = {\"text\": [examples[\"answer\"]] if isinstance(examples[\"answer\"], str) else examples[\"answer\"],\n",
    "           \"answer_start\": [0] * len(examples[\"answer\"])}\n",
    "    return {\n",
    "        \"id\": examples[\"question_id\"],\n",
    "        \"question\": examples[\"question\"],\n",
    "        \"context\": examples[\"document_title\"],\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "# Preprocess TriviaQA\n",
    "def preprocess_triviaqa(examples):\n",
    "    context = \", \".join(examples[\"entity_pages\"][\"wiki_context\"]) or \", \".join(examples[\"search_results\"][\"search_context\"])\n",
    "    answer_text = examples[\"answer\"][\"normalized_value\"]\n",
    "    answer_start = context.find(answer_text) if answer_text in context else -1\n",
    "    # answers = {\"text\": answer_text, \"answer_start\": [int(answer_start)]}  # Ensure answer_start is int32\n",
    "    answers = {\"text\": [answer_text], \"answer_start\": [int(answer_start)]}\n",
    "    return {\n",
    "        \"id\": examples[\"question_id\"],\n",
    "        \"question\": examples[\"question\"],\n",
    "        \"context\": context,\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "# Load datasets and preprocess\n",
    "squad = load_dataset(\"squad\")\n",
    "wikiqa = load_dataset(\"wiki_qa\")\n",
    "triviaqa = load_dataset(\"trivia_qa\", \"rc\")\n",
    "\n",
    "squad = squad.map(preprocess_squad)\n",
    "wikiqa = wikiqa.map(preprocess_wikiqa)\n",
    "triviaqa = triviaqa.map(preprocess_triviaqa)\n",
    "\n",
    "squad = squad.remove_columns([\"title\"])\n",
    "wikiqa = wikiqa.remove_columns([\"question_id\", \"document_title\", \"answer\", \"label\"])\n",
    "triviaqa = triviaqa.remove_columns(['question_id', 'question_source', 'entity_pages', 'search_results', 'answer'])\n",
    "\n",
    "# Ensure all datasets share the same features schema\n",
    "common_features = Features({\n",
    "    \"id\": Value(\"string\"),\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"context\": Value(\"string\"),\n",
    "    \"answers\": Sequence(\n",
    "        {\n",
    "            \"text\": Value(\"string\"),\n",
    "            \"answer_start\": Value(\"int32\")\n",
    "        }\n",
    "    )\n",
    "})\n",
    "\n",
    "print(squad[\"train\"].features)\n",
    "print(wikiqa[\"train\"].features)\n",
    "print(triviaqa[\"train\"].features)\n",
    "\n",
    "\n",
    "# Cast datasets to common schema\n",
    "squad = squad.cast(common_features)\n",
    "wikiqa = wikiqa.cast(common_features)\n",
    "triviaqa = triviaqa.cast(common_features)\n",
    "\n",
    "# Combine datasets\n",
    "# qa_datasets = DatasetDict({\n",
    "#     \"train\": concatenate_datasets([squad[\"train\"], wikiqa[\"train\"], triviaqa[\"train\"]]),\n",
    "#     \"validation\": concatenate_datasets([squad[\"validation\"], wikiqa[\"validation\"], triviaqa[\"validation\"]]),\n",
    "# })\n",
    "\n",
    "# print(qa_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# print(qa_datasets[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BO import iterative_loop, get_BO_plots\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "\n",
    "def run_BO(all_datasets, validation_dataset, iterations, num_epochs=20, printout=False):\n",
    "    print(\"running BO...\")\n",
    "    X, observations, gp = iterative_loop(all_datasets, validation_dataset, num_epochs=num_epochs, iterations=iterations, printout=printout)\n",
    "    BO_to_plot = get_BO_plots(observations) # BO results\n",
    "    naive_combine = BO_to_plot[0] # naive mixing result is the first iteration result of BO\n",
    "\n",
    "    # plot model performance as BO progresses...\n",
    "    plt.plot(range(len(BO_to_plot)), BO_to_plot, c=\"blue\", alpha=0.3, label=\"BO on mixing ratio\")\n",
    "    plt.axhline(naive_combine, linestyle=\"--\", c=\"red\", label=\"sample from each data source equally\")\n",
    "    plt.xlabel(\"BO iterations\")\n",
    "    plt.ylabel(\"accuracy on evaluation task\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot posterior\n",
    "    # posterior_acc = []\n",
    "    # for x in np.linspace(0,1,100):\n",
    "    #     posterior_acc.append(gp.posterior(torch.Tensor([[x,1-x]])).mean.item())\n",
    "        \n",
    "    # plt.plot(np.linspace(0,1,100), posterior_acc)\n",
    "    # plt.xlabel(\"mixing ratio (percentage on cats and dogs)\")\n",
    "    # plt.ylabel(\"accuracy\")\n",
    "    # plt.title(\"evaluation ratio : 1.0 cats and dogs\")\n",
    "    # plt.show()\n",
    "\n",
    "    def get_optimal_mixture_from_GP_posterior():\n",
    "        UCB = UpperConfidenceBound(gp, beta=0.0)\n",
    "        bounds = torch.stack([torch.zeros(len(all_datasets)), torch.ones(len(all_datasets))]) # need to change the bounds for parameters\n",
    "        A = [1.0] * len(all_datasets)\n",
    "        x = list(range(len(all_datasets)))\n",
    "        candidate, acq_value = optimize_acqf(\n",
    "            UCB, bounds=bounds, q=1, num_restarts=20, raw_samples=30,\n",
    "            equality_constraints = [(torch.tensor(x), torch.tensor(A), 1)]\n",
    "        )\n",
    "        return candidate\n",
    "    \n",
    "\n",
    "    def get_best_observation_mixture():\n",
    "        \n",
    "        # Find the index in list B that has the highest value\n",
    "        highest_index = observations.index(max(observations))\n",
    "        \n",
    "        # Return the corresponding item in list A\n",
    "        return X[highest_index]\n",
    "\n",
    "    \n",
    "    print(\"best mixture found in BO iterations is: \", get_best_observation_mixture())\n",
    "    \n",
    "    return X, observations, gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: SQuAD, WikiQA, TriviaQA\n",
      "test data ratio: SQuAD 70%, WikiQA 30%, TriviaQA 0%\n",
      "running BO...\n",
      "mixing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede7da8dc9c4cfb92d4740211f3508c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73901 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b38970ffbe46f783eb3c3dcae0e229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaddee29aab4420aa2aef246893f979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc56eedaffa04e31925bcdec3e5d3199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\FYP-QNA\\model_training.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dab8005d364c2ea92ffee9bf9ce311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# run BO on different mixtures of the training data. At each iteration, we train a model using the training data mixture and check the evaluation performance on validation dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mrun_BO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# should print a ratio that is close to evaluation_task_data_ratio\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mrun_BO\u001b[1;34m(all_datasets, validation_dataset, iterations, num_epochs, printout)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_BO\u001b[39m(all_datasets, validation_dataset, iterations, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, printout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning BO...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     X, observations, gp \u001b[38;5;241m=\u001b[39m \u001b[43miterative_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprintout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     BO_to_plot \u001b[38;5;241m=\u001b[39m get_BO_plots(observations) \u001b[38;5;66;03m# BO results\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     naive_combine \u001b[38;5;241m=\u001b[39m BO_to_plot[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# naive mixing result is the first iteration result of BO\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\BO.py:28\u001b[0m, in \u001b[0;36miterative_loop\u001b[1;34m(data_sources, validation_data, num_epochs, iterations, method, data, printout)\u001b[0m\n\u001b[0;32m     25\u001b[0m     mixed_data \u001b[38;5;241m=\u001b[39m get_data_from_mixing_ratio(data_sources, input_X) \u001b[38;5;66;03m# each agent do some influence function process to get data\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 28\u001b[0m     acc_all, observed_performance, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprintout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# observe the performance of this dataset from finetuning\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m printout:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperformance after training: \u001b[39m\u001b[38;5;124m\"\u001b[39m, observed_performance)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\model_training.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(all_dataset, test_dataset, seed, num_epochs, cuda, printout, lr, num_layer_to_unfreeze)\u001b[0m\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics  \u001b[38;5;66;03m# Pass the metric function\u001b[39;00m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m task_evaluator \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mevaluator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\accelerate\\accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\FYP-QNA\\env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helper import sample_from\n",
    "\n",
    "evaluation_task_data_ratio = 0.7\n",
    "print(\"train data: SQuAD, WikiQA, TriviaQA\")\n",
    "print(\"test data ratio: SQuAD {}%, WikiQA {}%, TriviaQA {}%\".format(round(evaluation_task_data_ratio * 100), round((1-evaluation_task_data_ratio)*100), 0))\n",
    "\n",
    "# data for training\n",
    "train_datasets = [squad[\"train\"], wikiqa[\"train\"], triviaqa[\"train\"]]\n",
    "\n",
    "# data just for evaluation; in real life we do not know this mixture. We can change this composition ratio or use other loaders\n",
    "# for example, this uses SQuAD and Wiki QA in a ratio of 70% and 30% in the evaluation task.\n",
    "validation_datasets = [squad[\"validation\"], wikiqa[\"validation\"], triviaqa[\"validation\"]]\n",
    "validation_ratio = [evaluation_task_data_ratio, 1-evaluation_task_data_ratio, 0]\n",
    "validation_dataset = sample_from(validation_datasets, validation_ratio, seed=2024)\n",
    "\n",
    "iterations=10\n",
    "# run BO on different mixtures of the training data. At each iteration, we train a model using the training data mixture and check the evaluation performance on validation dataset\n",
    "run_BO(train_datasets, validation_dataset, iterations, printout=True) # should print a ratio that is close to evaluation_task_data_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
